{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A.S. Lundervold, 25.10.2021"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When faced with a predictive task based on non-imaging data, an approach that has seen some success in certain cases, and therefore worthwhile knowing about and considering, is to find a way to **represent your data as images**. \n",
    "\n",
    "This makes it possible to use the relatively well-developed image analysis techniques in deep learning, f.ex. the landscape surrounding convolutional neural networks. \n",
    "\n",
    "Of course, this clearly won't make sense in _all_ cases. It can be hard--or impossible--to capture the information needed to make useful predictions in an image representation, in a way that's adapted to the image-based predictive models you'll want to use. \n",
    "\n",
    "Still, there are examples of this leading to good results:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An approach to **sound classification**, as used f.ex. for speech recognition, is based on turning audio into _spectrograms_, e.g. using mel spectrograms to represent speech: \n",
    "\n",
    "<img width=60% src=\"assets/melspecs.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Catching fraudsters using their mouse movements](https://www.splunk.com/en_us/blog/security/deep-learning-with-splunk-and-tensorflow-for-security-catching-the-fraudster-in-neural-networks-with-behavioral-biometrics.html) is another interesting example. So is this one: [malware classification](https://ieeexplore.ieee.org/abstract/document/8328749). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An example of the more general idea of representing **time series data to image data** from my own work is a way to represent **longitudinal measurements** recorded from multiple dementia subjects as greyscale images, one per subject:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img width=70% src=\"assets/long_images1.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"assets/long_images2.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook illustrates the approach on two quite different data sets and problems: \n",
    "\n",
    "1. Audio recordings and sound classification (this notebook)\n",
    "2. Small molecules and **prediction of bioactivity for given targets**, a core component of **drug discovery** (the next notebook)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is to get you thinking about whether transforming data into images and use image-based deep learning models could be useful in your own work. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: you may disregard these. Added to deal with PyTorch and CUDA version \n",
    "# issues on my computer\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a quick check of whether the notebook is currently running on Google Colaboratory, as that makes some difference for the code below.\n",
    "# We'll do this in every notebook of the course.\n",
    "if 'google.colab' in str(get_ipython()):\n",
    "    print('The notebook is running on Colab. colab=True.')\n",
    "    colab=True\n",
    "else:\n",
    "    print('The notebook is not running on Colab. colab=False.')\n",
    "    colab=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set to True if you're using Paperspace Gradient:\n",
    "gradient=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if colab or gradient:\n",
    "    !pip install -Uqq fastbook\n",
    "    !pip install -Uqq fastaudio\n",
    "    import fastbook\n",
    "    fastbook.setup_book()\n",
    "    from fastbook import *\n",
    "    from fastai.vision.all import *\n",
    "    NB_DIR = Path.cwd()\n",
    "else:\n",
    "    from fastai.vision.all import *\n",
    "    NB_DIR = Path.cwd()\n",
    "    DATA = Path('/data2/alex/pcs-data')  # Set this to where you want to store downloaded data  \n",
    "    \n",
    "if colab:\n",
    "    DATA = Path('./gdrive/MyDrive/ColabData')\n",
    "    DATA.mkdir(exist_ok=True)\n",
    "if gradient:\n",
    "    DATA = Path('/storage')\n",
    "    DATA.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, shutil, gc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sound classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"assets/ESC-50.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/karolpiczak/ESC-50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Note: the full data set is 600MB. You may want to use the sample data instead_. If you set `sample=True` below you will use a version of the ESC-50 data set that only has nine classes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not colab:\n",
    "\n",
    "    if sample:\n",
    "        path = untar_data('https://www.dropbox.com/s/iuq72ty8bu1v5nc/ecs50-sample.zip?dl=1', \n",
    "                          fname = 'data/ecs50-sample.zip', dest=DATA)\n",
    "    if not sample:\n",
    "        path = untar_data('https://github.com/karoldvl/ESC-50/archive/master.zip', \n",
    "                          fname=DATA/'master.zip', dest=DATA)\n",
    "        \n",
    "if colab:\n",
    "    if sample:\n",
    "        path = untar_data('https://www.dropbox.com/s/iuq72ty8bu1v5nc/ecs50-sample.zip?dl=1')\n",
    "    if not sample:\n",
    "        path = untar_data('https://github.com/karoldvl/ESC-50/archive/master.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path.ls()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio = path/'audio'\n",
    "if sample:\n",
    "    df = pd.read_csv(path/'meta'/'esc50-sample.csv')\n",
    "if not sample:\n",
    "    df = pd.read_csv(path/'meta'/'esc50.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio.ls()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.category.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Explore the data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The audio files are stored as WAV files. We can work with audio files in Python using the very convenient `librosa` library. \n",
    "\n",
    "<img src=\"https://librosa.org/images/librosa_logo_text.png\">\n",
    "\n",
    "> Below, we'll use the `fastaudio` library, which relies heavily on librosa. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import librosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "example_fn = audio.ls()[0]\n",
    "example_fn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "We can load the WAV file and store the waveform in `y` and the sampling rate in `sr`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "y, sr = librosa.load(example_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "`y` is then just a Numpy array, and we can work with it as such:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "`sr` is a number (extracted from the WAV file):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "sr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Here's a plot of `y`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "plt.plot(y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Using the sampling rate we can put the time on the x-axis. This is implemented by librosa's `waveplot`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from librosa.display import waveplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "waveplot(y, sr)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Here's a bunch of examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "for fn in df.sample(n=5).filename:\n",
    "    y, sr = librosa.load(audio/fn)\n",
    "    waveplot(y, sr)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "We can play the audio files via our notebooks using IPython:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import Audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "Audio(example_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def plot_play_clip(a, sr=None):\n",
    "    if type(a)!=np.ndarray:\n",
    "        print(f\"File {a.stem}\")\n",
    "        print(f\"Category: {df.loc[df.filename==f'{a.stem}.wav'].category.values}\")\n",
    "        a, sr = librosa.load(a, sr=sr)\n",
    "    waveplot(a, sr)\n",
    "    plt.show()\n",
    "    return Audio(a, rate=sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "plot_play_clip(audio.ls()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "plot_play_clip(audio.ls()[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "plot_play_clip(audio.ls()[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "plot_play_clip(audio.ls()[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "We can try to get a sense of the variation in the data by listening to some examples from specific categories:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def random_category_examples(c, k=3):\n",
    "    relevant = df.loc[df.category==c]\n",
    "    for i in range(k):\n",
    "        fn = relevant.iloc[i].filename\n",
    "    return relevant.sample(n=k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df.category.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "examples = random_category_examples('crow')\n",
    "examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "plot_play_clip(audio/examples.iloc[0].filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "plot_play_clip(audio/examples.iloc[1].filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "plot_play_clip(audio/examples.iloc[2].filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "All the audio files are 5 seconds long:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "librosa.get_duration(y, sr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "> This simplifies some things for us. If you're dealing with time series of different lengths you may consider cropping the series or splitting it up in parts. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Side-note: Audio as time series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "> These audio recordings are examples of (univariate) time series and can be studied and dealt with using what you learned about in time series analysis. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## (Re)sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "plot_play_clip(example_fn, sr=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "plot_play_clip(example_fn, sr=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "> All the various techniques and issues related to sampling of continuous signals applies. E.g. the [Nyquist-Shannon sampling theorem](https://en.wikipedia.org/wiki/Nyquist%E2%80%93Shannon_sampling_theorem) stating requirements and limitations for accurate capture of a signal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Combining time series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "list(examples.filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "ex1 = audio/list(examples.filename)[0]\n",
    "ex2 = audio/list(examples.filename)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "ex1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "ex2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "plot_play_clip(ex1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "plot_play_clip(ex2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "y1, sr1 = librosa.load(ex1)\n",
    "y2, sr2 = librosa.load(ex2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "y1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "y2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "y = y1 + y2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "plot_play_clip(y, sr=sr1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Time-domain versus frequency domain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<img src=\"https://i.stack.imgur.com/yrMqS.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "As you learned about in the time series module of the course, it is useful to work with time series in the time domain _and_ in the frequency domain. That's true also for audio. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "> From the time domain to the frequency domain and back."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### FFT: The fast fourier transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "y, sr = librosa.load(example_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "plot_play_clip(y, sr=sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "y_fft = scipy.fftpack.fft(y)\n",
    "y_fft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "plt.plot(abs(y_fft),'r') \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "scipy.fftpack.ifft(y_fft).real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Short-time fourier transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "y, sr = librosa.load(example_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "S = np.abs(librosa.stft(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "S.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "plot_play_clip(example_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "img = librosa.display.specshow(librosa.amplitude_to_db(S, ref=np.max), y_axis='log', x_axis='time', ax=ax)\n",
    "ax.set_title('Power spectrogram')\n",
    "fig.colorbar(img, ax=ax, format=\"%+2.0f dB\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Wavelets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "$$T(a,b) = \\frac{1}{\\sqrt a} \\int_{-\\infty}^{\\infty} x(t)\\psi^* \\frac{(t-b)}{a}dt$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "a: dilation parameter<br>\n",
    "b: location of wavelet<br>\n",
    "$\\psi$: wavelet function<br>\n",
    "x: signal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "So, now we know which frequencies exist in the time signal and where they exist."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "This is the advantage that wavelet transform has over FFT. It can capture spectral and temporal information simultaneously."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Morlet wavelet:\n",
    "\n",
    "$$\\psi(t) = \\exp(-i\\omega_0 t) \\exp(-t^2/2) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "t = np.linspace(-10.,10.,1000)\n",
    "w0 = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "psi = np.exp(-1j * w0 * t) * np.exp(-t**2 / 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "plt.plot(t, psi)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "https://github.com/AdityaDutt/Audio-Classification-Using-Wavelet-Transform/blob/main/wavelet_tutorial.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "**Computing wavelet coefficients**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "!{sys.executable} -m pip install PyWavelets scaleogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import pywt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import scaleogram as scg "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "wavelet = 'morl'\n",
    "widths = np.arange(1, 256)\n",
    "dt = 1 / sr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "frequencies = pywt.scale2frequency(wavelet, widths) / dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "frequencies.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "upper = ([x for x in range(len(widths)) if frequencies[x] > 1000])[-1]\n",
    "lower = ([x for x in range(len(widths)) if frequencies[x] < 80])[0]\n",
    "widths = widths[upper:lower] # Select scales in this frequency range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "wavelet_coeffs, freqs = pywt.cwt(y, widths, wavelet = wavelet, sampling_period=dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "wavelet_coeffs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "t, _ = np.linspace(0, 200, wavelet_coeffs.shape[1], retstep=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "wavelet_coeffs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Show some of the scales and some of the coefficients:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "plt.imshow(wavelet_coeffs[:,:800], cmap='coolwarm')\n",
    "plt.ylabel(\"Scales\")\n",
    "plt.yticks(widths[0::11])\n",
    "plt.title(\"Scalogram\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def get_wavelet_scalogram(a, wavelet = 'morl', plot_some=True):\n",
    "    y, sr = librosa.load(a)\n",
    "    widths = np.arange(1, 256)\n",
    "    dt = 1 / sr\n",
    "    frequencies = pywt.scale2frequency(wavelet, widths) / dt\n",
    "    \n",
    "    upper = ([x for x in range(len(widths)) if frequencies[x] > 1000])[-1]\n",
    "    lower = ([x for x in range(len(widths)) if frequencies[x] < 80])[0]\n",
    "    widths = widths[upper:lower]\n",
    "    \n",
    "    wavelet_coeffs, freqs = pywt.cwt(y, widths, wavelet = wavelet, sampling_period=dt)\n",
    "    t, _ = np.linspace(0, 200, wavelet_coeffs.shape[1], retstep=True)\n",
    "    plt.imsave(DATA/'wavimgs'/f'{a.stem}-img.png', wavelet_coeffs[:,:1000], cmap='coolwarm')\n",
    "    \n",
    "    if plot_some:\n",
    "        plt.imshow(wavelet_coeffs[:,:500], cmap='coolwarm')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "get_wavelet_scalogram(example_fn, plot_some=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Prepare the data: from WAV to spectograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "if colab or gradient:\n",
    "    !pip install fastaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from fastaudio.core.all import *\n",
    "from fastaudio.augment.all import *\n",
    "from fastaudio.ci import skip_if_ci"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "audio.ls()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "at = AudioTensor.create(audio.ls()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "at.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "?AudioConfig.BasicMelSpectrogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "cfg = AudioConfig.BasicMelSpectrogram(n_fft=512, sample_rate=sr)\n",
    "a2s = AudioToSpec.from_cfg(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "a2s.settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "pipeline = Pipeline([AudioTensor.create, a2s])\n",
    "\n",
    "a = AudioTensor.create(audio.ls()[0])\n",
    "a.show()\n",
    "pipeline(audio.ls()[0]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Split into train-val data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "To use the same cross-validation setup as in original paper: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "From https://colab.research.google.com/github/fastaudio/fastaudio/blob/master/docs/ESC50:%20Environmental%20Sound%20Classification.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def CrossValidationSplitter(col='fold', fold=1):\n",
    "    \"Split `items` (supposed to be a dataframe) by fold in `col`\"\n",
    "    def _inner(o):\n",
    "        col_values = o.iloc[:,col] if isinstance(col, int) else o[col]\n",
    "        valid_idx = (col_values == fold).values.astype('bool')\n",
    "        return IndexSplitter(mask2idxs(valid_idx))(o)\n",
    "    return _inner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Data loaders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "**Datablock:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "auds = DataBlock(blocks = (AudioBlock, CategoryBlock),  \n",
    "                 get_x = ColReader(\"filename\", pref=audio), \n",
    "                 splitter = CrossValidationSplitter(fold=1),\n",
    "                 batch_tfms = [a2s],\n",
    "                 get_y = ColReader(\"category\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "**Dataloader:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "dbunch = auds.dataloaders(df, bs=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "dbunch.show_batch(figsize=(10, 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Training a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "learn = cnn_learner(dbunch, \n",
    "            resnet34,\n",
    "            n_in=1, \n",
    "            loss_func=CrossEntropyLossFlat(),\n",
    "            metrics=[accuracy])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "lr = learn.lr_find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "learn.fit_one_cycle(11, lr.lr_steep)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Evaluate the results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Here are some predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "learn.show_results()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Let's count up the mistakes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "interp = ClassificationInterpretation.from_learner(learn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "interp.plot_confusion_matrix(figsize=(12,12))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Not too bad. How does this compare to other researchers working on the same data set? To answer that, we need to expand our evaluation somewhat as we should use the exact same data for evaluating our models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Comparing our results to those of others"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Let's make a function that performs the steps above, but on different validation folds (i.e. the same cross-validation setup as in the original paper):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def train_model(fold, auds=auds, epochs=12, bs=128, \n",
    "                item_tfms=None, n_in=1, callbacks = None):\n",
    "    \n",
    "    # Set up the data\n",
    "    auds = DataBlock(blocks = (AudioBlock, CategoryBlock),  \n",
    "                 get_x = ColReader(\"filename\", pref=audio), \n",
    "                 splitter = CrossValidationSplitter(fold=fold),\n",
    "                 item_tfms = item_tfms,\n",
    "                 batch_tfms = [a2s],\n",
    "                 get_y = ColReader(\"category\"))\n",
    "    dbunch = auds.dataloaders(df, bs=bs)\n",
    "   \n",
    "    # Create the model\n",
    "    learn = cnn_learner(dbunch, resnet34, n_in=n_in, \n",
    "                        loss_func=CrossEntropyLossFlat(), \n",
    "                        metrics=[accuracy], cbs=callbacks\n",
    "                       )\n",
    "    \n",
    "    # Find a learning rate\n",
    "    lr = learn.lr_find(show_plot=False)\n",
    "    \n",
    "    # Train the model\n",
    "    learn.fit_one_cycle(epochs, lr.lr_steep)\n",
    "    \n",
    "    # Return the accuracy of the model on the validation data\n",
    "    return learn.metrics[0].value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The training process takes some time. We can load the outputs as images below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# We train the model on all five folds specified by the creators of the data set\n",
    "accs = []\n",
    "for fold in df.fold.unique():\n",
    "    accs.append(train_model(fold, epochs=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import display, Image\n",
    "display(Image('https://github.com/alu042/PCS956-DL-2021/raw/master/2-intro_to_practical_dl/assets/audio_init_crossval_1.png'))\n",
    "display(Image('https://github.com/alu042/PCS956-DL-2021/raw/master/2-intro_to_practical_dl/assets/audio_init_crossval_2.png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "accs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Average accuracy across the folds:\n",
    "np.array(accs).sum()/len(df.fold.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "display(Image('https://github.com/alu042/PCS956-DL-2021/raw/master/2-intro_to_practical_dl/assets/audio_init_crossval_2.png'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "How does it compare to others? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "https://github.com/karolpiczak/ESC-50#results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "We're at the level of [this poster](https://www.mi.t.u-tokyo.ac.jp/assets/publication/LEARNING_ENVIRONMENTAL_SOUNDS_WITH_END-TO-END_CONVOLUTIONAL_NEURAL_NETWORK-poster.pdf) submitted to the 2017 IEEE International Conference on Acoustics, Speech and Signal Processing (71%). Not too bad for such a simple setup, but, still, quite far from the top... "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Improving our results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Data augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "There are many approaches one could pursue for data augmentation in audio. From simple things like adding various forms of noise at random, resampling at random, or changing the pitch and speed, to more advanced ideas like [masking blocks of time-steps or frequencies](https://arxiv.org/abs/1904.08779)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Let's try a few of those propsed in the paper linked above. They come built-in to `fastaudio`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "DBMelSpec = SpectrogramTransformer(mel=True, to_db=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "aud2spec = DBMelSpec(sample_rate=sr, n_mels=128, n_fft=512, hop_length=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "aud2spec.settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "item_tfms = [aud2spec, MaskTime(size=4), MaskFreq(size=10)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Here are some images showing the effect of the data augmentations (for the plots, we've made the data augmentation more pronounced for illustration purposes):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "auds.item_tfms = [aud2spec, MaskTime(size=30), MaskFreq(size=30)]\n",
    "dls = auds.dataloaders(df, bs=128)\n",
    "dls.show_batch(max_n=2, figsize=(8,8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "You notice the deleted temporal parts and the deleted frequencies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Note: Again, the training takes some time and we can rather load the outputs from a previous round."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# We train for a few additional epochs as we now have (in effect) more data\n",
    "accs_da = []\n",
    "for fold in df.fold.unique():\n",
    "    accs_da.append(train_model(fold, item_tfms=item_tfms, epochs=15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "accs_da"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Average accuracy across the folds:\n",
    "np.array(accs_da).sum()/len(df.fold.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import display, Image\n",
    "display(Image('https://github.com/alu042/PCS956-DL-2021/raw/master/2-intro_to_practical_dl/assets/audio_da_crossval_1.png'))\n",
    "display(Image('https://github.com/alu042/PCS956-DL-2021/raw/master/2-intro_to_practical_dl/assets/audio_da_crossval_2.png'))\n",
    "display(Image('https://github.com/alu042/PCS956-DL-2021/raw/master/2-intro_to_practical_dl/assets/audio_da_crossval_3.png'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "A solid improvement over our previous result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Our previous result:\n",
    "np.array(accs).sum()/len(df.fold.unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Adding additional images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Let's try to add _delta_ features as new images. They are computed using an estimate of the _derivatives_ of the first and second order in our signal. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "item_tfms = [aud2spec, MaskTime(size=4), MaskFreq(size=10), Delta()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Here's the effect of the Delta transformations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "auds.item_tfms = item_tfms\n",
    "dls = auds.dataloaders(df, bs=128)\n",
    "dls.show_batch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "We now have three channels instead of one. The original image in channel 0, then the two images obtained from the (approximate) derivatives in the time and the frequency directions in channels 1 and 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Let's check if the information from the representations derived from Delta helps our models:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "> The training takes a while. You can rather load the training outputs as images below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# We train for a few additional epochs as we now have (in effect) more data\n",
    "accs_delta = []\n",
    "for fold in df.fold.unique():\n",
    "    accs_delta.append(train_model(fold, item_tfms=item_tfms, epochs=18, n_in=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "accs_delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Average accuracy across the folds:\n",
    "np.array(accs_delta).sum()/len(df.fold.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from IPython.display import display, Image\n",
    "for fn in sorted(list((NB_DIR/'assets'/'audio_training').iterdir())):\n",
    "    display(Image(fn))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "This result is competitive with the 79.80% obtained in the paper [Look, Listen and Learn](https://openaccess.thecvf.com/content_iccv_2017/html/Arandjelovic_Look_Listen_and_ICCV_2017_paper.html) from ICCV2017 by the two Google DeepMind researchers Relja Arandjelović and Andrew Zisserman (who's also a relatively famous researcher in the VGG group at the University of Oxford). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Human performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "In the paper introducing the ECS-50 data set they reported an experiment where they crowdsourced labels from human listeners. The reported human accuracy was **81,30%** (in their somewhat limited experiment; read [the paper](https://www.karolpiczak.com/papers/Piczak2015-ESC-Dataset.pdf) for more details). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### The state-of-the-art"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "https://github.com/YuanGongND/ast claims an accuracy of 95,70% on the ECS-50 data set. They used _Transformers_, which is increasingly common among many state of the art models in deep learning. We'll learn about transformers next week..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### An important warning / caveat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The ECS-50 evaluation setup suffers from a very common problem across many machine learning data sets used to benchmark and compare models. As _there is no test data that the researchers cannot access until their model's generalization performance is estimated_ it is very easy to fool yourself into thinking you have better results than you _actually_ have. \n",
    "\n",
    "As an example, here's the same model as above trained using what's called _early stopping_, i.e. stopping the training when the validation loss is smallest (or close to smallest). This approach is the same as picking the model from the epoch where the validation loss was smallest during the above training procedure. If we look at the accuracies we see that we would then be able to make the (invalid) accuracy claim of **80.6%**.\n",
    "\n",
    "Another way to obtain invalid overestimates of the generalization performance would be to simply run the training above over and over, and then, because of randomness in many parts of the setup (e.g. randomness in initialization of the network weights, randomness in the data augmentation), find a cross-validation run that's better than the one we reported. \n",
    "\n",
    "You could also play with various hyperparameters of the model or the data augmentation setup, or simply try out a bunch of different number of epochs until you improve the results. \n",
    "\n",
    "> Again, I'm not recommending that you do this! It completely invalidates your results!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "This issue, which is, again, quite common across machine learning data sets used in research, means that _it should be a requirement that all the source code needed to fully reproduce the results is published along with the papers_. \n",
    "\n",
    "> Note: even if the code is shared you should be vigilant about any sign of tinkering to obtain what seems to be better results but aren't. Like fixed random seeds using `random_state` or similar... Instabilities in the validation loss during the latter parts of the training process, meaning that it fluctuates significantly, can also be a worrying sign. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Your turn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "> **Your turn!** Several of the tricks presented in the previous notebook can be applied. For example, would ensembling a few models improve the results? What about test-time augmentation?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "> **Your turn!** Can you incorporate the wavelet images as another input to the model? Does it help?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The next two notebooks will present some additional ideas that you may want to consider applying also to this problem."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pcs956-fastaudio",
   "language": "python",
   "name": "pcs956-fastaudio"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
